{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9f61ab",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains a series of code cells that demonstrate the process of loading a pre-trained Densenet121 model, preprocessing a sample image, using the model to predict the class of the image, converting the model to TFLite format, and testing the TFLite model with the sample image. Additionally, it includes code for downloading and preparing an image from a URL for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2cd839-f5f3-406d-9f98-16056d90d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26196ef5-b9b8-44ed-b375-1eb405dc9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../model/densenet121_v1_34_0.592.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c18d06-4cbd-4ce3-aad0-2369238d74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "# Load the image\n",
    "img_path = '../data/clean-cifar-100-python/test-deployment/image_1.jpg' \n",
    "img = image.load_img(img_path, target_size=(75, 75))\n",
    "# Preprocess the image\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor /= 255.  # the model was trained on inputs that were normalized in the range [0, 1]\n",
    "\n",
    "# Use the model to predict the class of the image\n",
    "prediction = model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca24ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "# Load the image\n",
    "img_path = '../data/clean-cifar-100-python/test-deployment/image_1.jpg' \n",
    "img = image.load_img(img_path, target_size=(75, 75))\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_tensor = preprocess_input(img_array)\n",
    "\n",
    "# Use the model to predict the class of the image\n",
    "prediction = model.predict(img_tensor)\n",
    "\n",
    "# Add label names to the prediction probabilities\n",
    "class_names = ['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', \n",
    "               'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things',\n",
    "               'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', \n",
    "               'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc749a44-6fe3-4ab5-99ec-3671e1a0b1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aquatic_mammals': -6.5015044,\n",
       " 'fish': -20.108147,\n",
       " 'flowers': -1.783258,\n",
       " 'food_containers': -2.350761,\n",
       " 'fruit_and_vegetables': -1.9533087,\n",
       " 'household_electrical_devices': -8.131875,\n",
       " 'household_furniture': -8.4431,\n",
       " 'insects': 2.8202362,\n",
       " 'large_carnivores': 13.564195,\n",
       " 'large_man-made_outdoor_things': -15.193861,\n",
       " 'large_natural_outdoor_scenes': -9.958817,\n",
       " 'large_omnivores_and_herbivores': 4.116876,\n",
       " 'medium_mammals': 3.5384219,\n",
       " 'non-insect_invertebrates': 0.19212708,\n",
       " 'people': -14.425937,\n",
       " 'reptiles': -12.128974,\n",
       " 'small_mammals': 9.810526,\n",
       " 'trees': -7.0088897,\n",
       " 'vehicles_1': -11.2750635,\n",
       " 'vehicles_2': -11.919313}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(class_names, prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7792847-1b1e-4aa9-aefd-e7a07d48ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 120). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Ahmad\\AppData\\Local\\Temp\\tmpdzin7btf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Ahmad\\AppData\\Local\\Temp\\tmpdzin7btf\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('cifar-100-model.tflite', 'wb') as f_out:\n",
    "    f_out.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8602a513-5d90-4c0b-8729-42ae5554dd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.595596313476562"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.getsize(\"cifar-100-model.tflite\") / (1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d139d38-cf0c-4d85-94bd-008cf0977d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.lite as tflite\n",
    "interpreter = tflite.Interpreter(model_path='cifar-100-model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "output_index = interpreter.get_output_details()[0]['index']\n",
    "output_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3ef53e-4c26-4b48-ba7f-c0470f4af12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34563ec7-2373-4c66-bc55-90f7dd0adad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://i.imgur.com/23MiAxv.jpg'\n",
    "img = download_image(url)\n",
    "target_size = (75, 75)\n",
    "prep_img = prepare_image(img, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b320f11-7f35-46eb-84d3-7b409637f57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prep_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d45ff48-f01b-4fb2-906a-fcfed8a8f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R value of the first pixel is 6\n"
     ]
    }
   ],
   "source": [
    "pix = prep_img.load()\n",
    "\n",
    "# Get the RGB values of the first pixel\n",
    "r, g, b = pix[0, 0]\n",
    "\n",
    "print(f\"The R value of the first pixel is {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819f3c83-cb70-418e-96d3-3fe3f634354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023529411764705882\n"
     ]
    }
   ],
   "source": [
    "normalized_r = r / 255\n",
    "print(normalized_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c66d93-d18c-4e72-aaa2-63412643b309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aquatic_mammals': -22.021074,\n",
       " 'fish': -33.166653,\n",
       " 'flowers': -15.633723,\n",
       " 'food_containers': -6.470115,\n",
       " 'fruit_and_vegetables': -9.440056,\n",
       " 'household_electrical_devices': -2.7160254,\n",
       " 'household_furniture': 1.8643621,\n",
       " 'insects': 29.163816,\n",
       " 'large_carnivores': 33.332577,\n",
       " 'large_man-made_outdoor_things': -20.783209,\n",
       " 'large_natural_outdoor_scenes': -21.852894,\n",
       " 'large_omnivores_and_herbivores': -17.849588,\n",
       " 'medium_mammals': 4.1026273,\n",
       " 'non-insect_invertebrates': -1.5337784,\n",
       " 'people': -9.650715,\n",
       " 'reptiles': -20.151258,\n",
       " 'small_mammals': -8.165364,\n",
       " 'trees': -47.12472,\n",
       " 'vehicles_1': -13.878261,\n",
       " 'vehicles_2': 4.4217033}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the image\n",
    "url = 'https://i.imgur.com/23MiAxv.jpg'\n",
    "\n",
    "'''\n",
    "# Send a HTTP request to the URL of the image\n",
    "response = requests.get(url)\n",
    "\n",
    "# Open the URL image as a PIL image object\n",
    "img = Image.open(BytesIO(response.content))\n",
    "'''\n",
    "\n",
    "# Download the image from a url\n",
    "img = download_image(url)\n",
    "\n",
    "# Preprocess the image\n",
    "target_size = (75, 75)\n",
    "img = prepare_image(img, target_size) # Resize the image\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor = preprocess_input(img_tensor)\n",
    "\n",
    "# Now use the .tflite model\n",
    "interpreter = tflite.Interpreter(model_path='cifar-100-model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "interpreter.set_tensor(input_index, img_tensor)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)\n",
    "\n",
    "# Print the prediction\n",
    "dict(zip(class_names, preds[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07337509-5a24-43f8-a69b-8223e1362a2c",
   "metadata": {},
   "source": [
    "To test the lambda function script locally in IPython:\n",
    "\n",
    "from lambda_function import lambda_handler\n",
    "event = {'url': 'https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg'}\n",
    "result = lambda_handler(event, None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24b608",
   "metadata": {},
   "source": [
    "To build the docker image for hw#09 Q6:\n",
    "\n",
    "```bash\n",
    "docker build -t ml-zoom-09 .\n",
    "```\n",
    "\n",
    "To run the docker container for hw#09 Q6:\n",
    "\n",
    "```bash\n",
    "docker run -p 9000:8080 -it ml-zoom-09\n",
    "```\n",
    "\n",
    "To test the lambda function in the docker container locally:\n",
    "\n",
    "```bash\n",
    "$ curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\":\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```json\n",
    "{\"Bee\": 0.5546649396419525, \"Wasp\": 0.4453350603580475}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
